{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpPfQrczyoZl"
   },
   "source": [
    "# Bi-LSTM pour l'Ã©tiquetage morpho-syntaxique\n",
    "Application du tutoriel https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UV5X-wxdv4n0",
    "outputId": "bb80492a-68a6-4fff-c58e-3d7ac291d406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"treebank\")\n",
    " \n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyTHD0sPwVF8",
    "outputId": "4ce96824-0838-4895-8100-c40f353fae20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
      " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
      " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
      " '.']\n",
      "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
      " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "# Let's see how a sequence looks\n",
    " \n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A1i4llJHwY29"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    " \n",
    "(train_sentences, \n",
    " test_sentences, \n",
    " train_tags, \n",
    " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f720hcZCwcgk"
   },
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XbuZ1j8whg9",
    "outputId": "2af8937b-6111-48cd-8041-32b9031d8e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8515, 2721, 4067, 3457, 10094, 4067, 2688, 10152, 1335, 1667, 7608, 5032, 8330, 1419, 1485, 5791, 6987, 853, 3274, 390, 4067, 8430, 5223, 8931, 4067, 6987, 1253, 3113, 2576, 7128, 9462, 7472, 2721, 4067, 6987, 516, 2217, 7284]\n",
      "[1, 1, 1182, 4375, 2152, 8648, 1, 1, 7727, 390, 7758, 9120, 4067, 7458, 9038, 5032, 1, 1279, 7284]\n",
      "[6, 6, 10, 1, 23, 10, 14, 14, 26, 12, 7, 31, 12, 1, 7, 6, 14, 30, 1, 6, 10, 1, 5, 40, 10, 14, 6, 41, 25, 1, 6, 30, 6, 10, 14, 25, 30, 18]\n",
      "[14, 14, 43, 14, 27, 10, 25, 14, 30, 44, 7, 27, 10, 30, 7, 31, 12, 1, 18]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfhTYNNDwpDD",
    "outputId": "d2e006e9-735f-486e-f321-aed3fb4d7409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hdNld_qwuaj",
    "outputId": "0d3c2a0e-2216-4d42-b89b-bd3cad833981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8515  2721  4067  3457 10094  4067  2688 10152  1335  1667  7608  5032\n",
      "  8330  1419  1485  5791  6987   853  3274   390  4067  8430  5223  8931\n",
      "  4067  6987  1253  3113  2576  7128  9462  7472  2721  4067  6987   516\n",
      "  2217  7284     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n",
      "[   1    1 1182 4375 2152 8648    1    1 7727  390 7758 9120 4067 7458\n",
      " 9038 5032    1 1279 7284    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[ 6  6 10  1 23 10 14 14 26 12  7 31 12  1  7  6 14 30  1  6 10  1  5 40\n",
      " 10 14  6 41 25  1  6 30  6 10 14 25 30 18  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[14 14 43 14 27 10 25 14 30 44  7 27 10 30  7 31 12  1 18  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWiaFgFpw6Dv",
    "outputId": "c84b81f9-7d38-4cd3-9b45-52dd7378f3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 271, 128)          1302400   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 271, 512)         788480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 271, 47)          24111     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 271, 47)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,114,991\n",
      "Trainable params: 2,114,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JqN1L4VQw-Js"
   },
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CulQ-41Jw_Cb",
    "outputId": "668cc52f-8295-4782-b50e-3ccfa420bbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5q7fs811xBgq",
    "outputId": "8376c152-4055-4ff7-d663-7d42cc4f8dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20/20 [==============================] - 128s 6s/step - loss: 1.2140 - accuracy: 0.8578 - val_loss: 0.3867 - val_accuracy: 0.9049\n",
      "Epoch 2/40\n",
      "20/20 [==============================] - 123s 6s/step - loss: 0.3381 - accuracy: 0.9053 - val_loss: 0.3237 - val_accuracy: 0.9050\n",
      "Epoch 3/40\n",
      "20/20 [==============================] - 132s 7s/step - loss: 0.3171 - accuracy: 0.9083 - val_loss: 0.3137 - val_accuracy: 0.9156\n",
      "Epoch 4/40\n",
      "20/20 [==============================] - 121s 6s/step - loss: 0.3078 - accuracy: 0.9161 - val_loss: 0.3051 - val_accuracy: 0.9161\n",
      "Epoch 5/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.2992 - accuracy: 0.9168 - val_loss: 0.2980 - val_accuracy: 0.9164\n",
      "Epoch 6/40\n",
      "20/20 [==============================] - 124s 6s/step - loss: 0.2909 - accuracy: 0.9174 - val_loss: 0.2920 - val_accuracy: 0.9177\n",
      "Epoch 7/40\n",
      "20/20 [==============================] - 131s 7s/step - loss: 0.2833 - accuracy: 0.9193 - val_loss: 0.2854 - val_accuracy: 0.9192\n",
      "Epoch 8/40\n",
      "20/20 [==============================] - 130s 6s/step - loss: 0.2781 - accuracy: 0.9204 - val_loss: 0.2801 - val_accuracy: 0.9196\n",
      "Epoch 9/40\n",
      "20/20 [==============================] - 1924s 101s/step - loss: 0.2735 - accuracy: 0.9217 - val_loss: 0.2757 - val_accuracy: 0.9247\n",
      "Epoch 10/40\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.2687 - accuracy: 0.9249 - val_loss: 0.2688 - val_accuracy: 0.9279\n",
      "Epoch 11/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.2632 - accuracy: 0.9333 - val_loss: 0.2613 - val_accuracy: 0.9361\n",
      "Epoch 12/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.2545 - accuracy: 0.9396 - val_loss: 0.2503 - val_accuracy: 0.9398\n",
      "Epoch 13/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.2403 - accuracy: 0.9429 - val_loss: 0.2344 - val_accuracy: 0.9435\n",
      "Epoch 14/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.2208 - accuracy: 0.9464 - val_loss: 0.2141 - val_accuracy: 0.9477\n",
      "Epoch 15/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.1992 - accuracy: 0.9511 - val_loss: 0.1943 - val_accuracy: 0.9507\n",
      "Epoch 16/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.1781 - accuracy: 0.9558 - val_loss: 0.1750 - val_accuracy: 0.9566\n",
      "Epoch 17/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.1575 - accuracy: 0.9604 - val_loss: 0.1557 - val_accuracy: 0.9604\n",
      "Epoch 18/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.1373 - accuracy: 0.9655 - val_loss: 0.1384 - val_accuracy: 0.9653\n",
      "Epoch 19/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.1184 - accuracy: 0.9709 - val_loss: 0.1219 - val_accuracy: 0.9699\n",
      "Epoch 20/40\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.1008 - accuracy: 0.9763 - val_loss: 0.1068 - val_accuracy: 0.9733\n",
      "Epoch 21/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0844 - accuracy: 0.9809 - val_loss: 0.0923 - val_accuracy: 0.9779\n",
      "Epoch 22/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.0695 - accuracy: 0.9851 - val_loss: 0.0806 - val_accuracy: 0.9813\n",
      "Epoch 23/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0575 - accuracy: 0.9882 - val_loss: 0.0720 - val_accuracy: 0.9834\n",
      "Epoch 24/40\n",
      "20/20 [==============================] - 265s 14s/step - loss: 0.0475 - accuracy: 0.9905 - val_loss: 0.0641 - val_accuracy: 0.9850\n",
      "Epoch 25/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.0397 - accuracy: 0.9921 - val_loss: 0.0580 - val_accuracy: 0.9862\n",
      "Epoch 26/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0336 - accuracy: 0.9932 - val_loss: 0.0541 - val_accuracy: 0.9869\n",
      "Epoch 27/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0288 - accuracy: 0.9942 - val_loss: 0.0507 - val_accuracy: 0.9876\n",
      "Epoch 28/40\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.0249 - accuracy: 0.9950 - val_loss: 0.0472 - val_accuracy: 0.9883\n",
      "Epoch 29/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0218 - accuracy: 0.9957 - val_loss: 0.0458 - val_accuracy: 0.9886\n",
      "Epoch 30/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0192 - accuracy: 0.9962 - val_loss: 0.0449 - val_accuracy: 0.9889\n",
      "Epoch 31/40\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.0170 - accuracy: 0.9967 - val_loss: 0.0427 - val_accuracy: 0.9893\n",
      "Epoch 32/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0153 - accuracy: 0.9970 - val_loss: 0.0429 - val_accuracy: 0.9894\n",
      "Epoch 33/40\n",
      "20/20 [==============================] - 119s 6s/step - loss: 0.0137 - accuracy: 0.9974 - val_loss: 0.0414 - val_accuracy: 0.9897\n",
      "Epoch 34/40\n",
      "20/20 [==============================] - 120s 6s/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.0398 - val_accuracy: 0.9901\n",
      "Epoch 35/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0113 - accuracy: 0.9978 - val_loss: 0.0399 - val_accuracy: 0.9899\n",
      "Epoch 36/40\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.0385 - val_accuracy: 0.9903\n",
      "Epoch 37/40\n",
      "20/20 [==============================] - 118s 6s/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.0401 - val_accuracy: 0.9901\n",
      "Epoch 38/40\n",
      "20/20 [==============================] - 117s 6s/step - loss: 0.0089 - accuracy: 0.9983 - val_loss: 0.0389 - val_accuracy: 0.9903\n",
      "Epoch 39/40\n",
      "20/20 [==============================] - 1656s 87s/step - loss: 0.0082 - accuracy: 0.9983 - val_loss: 0.0382 - val_accuracy: 0.9904\n",
      "Epoch 40/40\n",
      "20/20 [==============================] - 251059s 13213s/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14ef648e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be2UQ-t_xRlt",
    "outputId": "83facce9-5ef3-435a-d66d-e760c2a5e0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 22s 891ms/step - loss: 0.0361 - accuracy: 0.9911\n",
      "accuracy: 99.10553097724915\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NErB-qCTxfF7",
    "outputId": "9ee9edbf-839d-4336-9d73-b337e3d31edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_ERjcBOxhcB",
    "outputId": "7c18d30c-8e80-40fa-e1d9-6b45a0d98b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7553 7224 5906  459 4987  689 7284    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [3325 3969 7553 4753  761 4987 8648 6416 7284    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WoS13jrxk6-",
    "outputId": "c445246e-57a0-41f3-c873-189f879eaa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[2.18290277e-03 1.93107054e-02 2.30315495e-02 ... 2.40547908e-03\n",
      "   7.02051399e-03 1.68226357e-03]\n",
      "  [9.15750570e-06 6.66147462e-05 2.70896533e-04 ... 2.05056864e-07\n",
      "   2.27354365e-04 5.29999261e-06]\n",
      "  [1.71216088e-04 2.22155518e-06 3.99776618e-04 ... 7.46191836e-06\n",
      "   4.49165556e-04 1.65691727e-05]\n",
      "  ...\n",
      "  [9.99987960e-01 2.31973889e-08 3.66844603e-07 ... 5.10959830e-10\n",
      "   1.69985265e-06 2.70491007e-08]\n",
      "  [9.99980330e-01 1.51965160e-08 5.13104737e-07 ... 1.59485392e-09\n",
      "   3.21016682e-06 4.85386771e-08]\n",
      "  [9.99967813e-01 1.15583942e-08 7.00975249e-07 ... 4.24078106e-09\n",
      "   5.57118392e-06 8.04772995e-08]]\n",
      "\n",
      " [[3.40510014e-04 7.23261561e-04 1.00592431e-03 ... 2.07038550e-03\n",
      "   5.51617821e-04 2.38451321e-04]\n",
      "  [1.32522528e-05 1.19818076e-04 3.93283699e-04 ... 1.53770301e-07\n",
      "   1.35581198e-04 9.28573627e-06]\n",
      "  [8.75950092e-04 1.41816388e-04 2.30100248e-02 ... 1.94781198e-04\n",
      "   9.16466210e-03 5.44423005e-04]\n",
      "  ...\n",
      "  [9.99987960e-01 2.31973889e-08 3.66844603e-07 ... 5.10959830e-10\n",
      "   1.69985265e-06 2.70491007e-08]\n",
      "  [9.99980330e-01 1.51965160e-08 5.13104283e-07 ... 1.59485392e-09\n",
      "   3.21016682e-06 4.85386771e-08]\n",
      "  [9.99967813e-01 1.15583942e-08 7.00975249e-07 ... 4.24078905e-09\n",
      "   5.57118938e-06 8.04772995e-08]]] (2, 271, 47)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZVsZlU6Nxnmk"
   },
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDlzd909xoq-",
    "outputId": "2962b87c-5db6-4683-bc2a-5836b69c3f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r-XdT-dUxueu"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNHBN2PBxve-",
    "outputId": "09ca19eb-1198-4cb8-de93-69077fc89be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 271, 128)          1290624   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,103,215\n",
      "Trainable params: 2,103,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83cb06OzxyG2",
    "outputId": "db8234a3-45b6-497a-cbec-70956158ae49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amal Zouaq\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 1.1602 - accuracy: 0.9021 - ignore_accuracy: 0.0291 - val_loss: 0.3754 - val_accuracy: 0.9114 - val_ignore_accuracy: 0.1228\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.3380 - accuracy: 0.9084 - ignore_accuracy: 0.0725 - val_loss: 0.3252 - val_accuracy: 0.9072 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 265s 106ms/step - loss: 0.3154 - accuracy: 0.9097 - ignore_accuracy: 0.0999 - val_loss: 0.3039 - val_accuracy: 0.9191 - val_ignore_accuracy: 0.1367\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.3047 - accuracy: 0.9162 - ignore_accuracy: 0.1326 - val_loss: 0.3083 - val_accuracy: 0.9190 - val_ignore_accuracy: 0.1374\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2959 - accuracy: 0.9170 - ignore_accuracy: 0.1356 - val_loss: 0.3019 - val_accuracy: 0.9190 - val_ignore_accuracy: 0.1377\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.2919 - accuracy: 0.9168 - ignore_accuracy: 0.1341 - val_loss: 0.3018 - val_accuracy: 0.9191 - val_ignore_accuracy: 0.1387\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2842 - accuracy: 0.9179 - ignore_accuracy: 0.1419 - val_loss: 0.3001 - val_accuracy: 0.9212 - val_ignore_accuracy: 0.1614\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.2769 - accuracy: 0.9208 - ignore_accuracy: 0.1692 - val_loss: 0.2871 - val_accuracy: 0.9257 - val_ignore_accuracy: 0.2065\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.2706 - accuracy: 0.9239 - ignore_accuracy: 0.2004 - val_loss: 0.2815 - val_accuracy: 0.9273 - val_ignore_accuracy: 0.2246\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.2639 - accuracy: 0.9317 - ignore_accuracy: 0.2835 - val_loss: 0.2754 - val_accuracy: 0.9368 - val_ignore_accuracy: 0.3303\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2540 - accuracy: 0.9392 - ignore_accuracy: 0.3625 - val_loss: 0.2628 - val_accuracy: 0.9424 - val_ignore_accuracy: 0.3913\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 262s 105ms/step - loss: 0.2388 - accuracy: 0.9456 - ignore_accuracy: 0.4301 - val_loss: 0.2246 - val_accuracy: 0.9484 - val_ignore_accuracy: 0.4488\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.2170 - accuracy: 0.9490 - ignore_accuracy: 0.4644 - val_loss: 0.2014 - val_accuracy: 0.9493 - val_ignore_accuracy: 0.4575\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.1923 - accuracy: 0.9512 - ignore_accuracy: 0.4882 - val_loss: 0.1782 - val_accuracy: 0.9537 - val_ignore_accuracy: 0.5047\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 267s 106ms/step - loss: 0.1687 - accuracy: 0.9563 - ignore_accuracy: 0.5418 - val_loss: 0.1578 - val_accuracy: 0.9590 - val_ignore_accuracy: 0.5623\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.1478 - accuracy: 0.9618 - ignore_accuracy: 0.6008 - val_loss: 0.1393 - val_accuracy: 0.9633 - val_ignore_accuracy: 0.6071\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 267s 106ms/step - loss: 0.1304 - accuracy: 0.9666 - ignore_accuracy: 0.6533 - val_loss: 0.1239 - val_accuracy: 0.9689 - val_ignore_accuracy: 0.6674\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.1117 - accuracy: 0.9727 - ignore_accuracy: 0.7143 - val_loss: 0.1081 - val_accuracy: 0.9729 - val_ignore_accuracy: 0.7110\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0942 - accuracy: 0.9778 - ignore_accuracy: 0.7674 - val_loss: 0.0928 - val_accuracy: 0.9771 - val_ignore_accuracy: 0.7546\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0775 - accuracy: 0.9828 - ignore_accuracy: 0.8197 - val_loss: 0.0793 - val_accuracy: 0.9815 - val_ignore_accuracy: 0.8030\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 273s 109ms/step - loss: 0.0631 - accuracy: 0.9871 - ignore_accuracy: 0.8656 - val_loss: 0.0679 - val_accuracy: 0.9850 - val_ignore_accuracy: 0.8404\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0512 - accuracy: 0.9903 - ignore_accuracy: 0.8985 - val_loss: 0.0590 - val_accuracy: 0.9870 - val_ignore_accuracy: 0.8616\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.0417 - accuracy: 0.9921 - ignore_accuracy: 0.9172 - val_loss: 0.0523 - val_accuracy: 0.9882 - val_ignore_accuracy: 0.8746\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.0345 - accuracy: 0.9934 - ignore_accuracy: 0.9312 - val_loss: 0.0473 - val_accuracy: 0.9890 - val_ignore_accuracy: 0.8838\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.0289 - accuracy: 0.9944 - ignore_accuracy: 0.9411 - val_loss: 0.0436 - val_accuracy: 0.9896 - val_ignore_accuracy: 0.8898\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0247 - accuracy: 0.9952 - ignore_accuracy: 0.9494 - val_loss: 0.0409 - val_accuracy: 0.9900 - val_ignore_accuracy: 0.8939\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0214 - accuracy: 0.9957 - ignore_accuracy: 0.9553 - val_loss: 0.0387 - val_accuracy: 0.9904 - val_ignore_accuracy: 0.8982\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0187 - accuracy: 0.9962 - ignore_accuracy: 0.9600 - val_loss: 0.0378 - val_accuracy: 0.9905 - val_ignore_accuracy: 0.9000\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0167 - accuracy: 0.9965 - ignore_accuracy: 0.9629 - val_loss: 0.0363 - val_accuracy: 0.9908 - val_ignore_accuracy: 0.9024\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.0150 - accuracy: 0.9969 - ignore_accuracy: 0.9674 - val_loss: 0.0349 - val_accuracy: 0.9911 - val_ignore_accuracy: 0.9056\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.0135 - accuracy: 0.9971 - ignore_accuracy: 0.9697 - val_loss: 0.0348 - val_accuracy: 0.9911 - val_ignore_accuracy: 0.9069\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0124 - accuracy: 0.9974 - ignore_accuracy: 0.9726 - val_loss: 0.0336 - val_accuracy: 0.9914 - val_ignore_accuracy: 0.9093\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0113 - accuracy: 0.9977 - ignore_accuracy: 0.9754 - val_loss: 0.0329 - val_accuracy: 0.9916 - val_ignore_accuracy: 0.9105\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0104 - accuracy: 0.9978 - ignore_accuracy: 0.9776 - val_loss: 0.0331 - val_accuracy: 0.9915 - val_ignore_accuracy: 0.9091\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 300s 120ms/step - loss: 0.0098 - accuracy: 0.9980 - ignore_accuracy: 0.9789 - val_loss: 0.0325 - val_accuracy: 0.9917 - val_ignore_accuracy: 0.9123\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 307s 123ms/step - loss: 0.0090 - accuracy: 0.9982 - ignore_accuracy: 0.9808 - val_loss: 0.0323 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9140\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 286s 114ms/step - loss: 0.0083 - accuracy: 0.9983 - ignore_accuracy: 0.9821 - val_loss: 0.0318 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9140\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 285s 114ms/step - loss: 0.0077 - accuracy: 0.9985 - ignore_accuracy: 0.9839 - val_loss: 0.0319 - val_accuracy: 0.9920 - val_ignore_accuracy: 0.9144\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0072 - accuracy: 0.9985 - ignore_accuracy: 0.9847 - val_loss: 0.0323 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9150\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0068 - accuracy: 0.9986 - ignore_accuracy: 0.9858 - val_loss: 0.0322 - val_accuracy: 0.9920 - val_ignore_accuracy: 0.9146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x115348f1f48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tv3fLOy9x0Lj",
    "outputId": "db220ae7-fcbc-4bc9-8502-0016d5456111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NNS', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 52s 66ms/step\n",
      "ignore_accuracy: 90.68601727485657\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore_accuracy: 90.68601727485657\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.metrics_names[2]}: {scores[2] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
